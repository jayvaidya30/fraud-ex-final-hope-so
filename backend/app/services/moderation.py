"""Safeguards & moderation layer.

Will enforce policy: prevent personal accusations in generated content.
"""
import re

DISCLAIMER = "\n\n> ⚠️ **DISCLAIMER**: This report is generated by an AI system. It indicates potential anomalies and risk indicators based on available data. It is NOT a conclusion of guilt or corruption. Human verification is required."

UNSAFE_KEYWORDS = [
    "guilty", "criminal", "bribe", "corrupt person", "stole", "fraudster"
]

def check_content_safety(text: str) -> bool:
    """
    Checks for unsafe content (accusations, PII, etc).
    Returns False if unsafe content is detected.
    """
    text_lower = text.lower()
    for word in UNSAFE_KEYWORDS:
        if f" {word} " in text_lower:
            return False
    return True

def sanitize_output(text: str) -> str:
    """
    Adds disclaimers and sanitizes language.
    """
    # Simple sanitization: replace strong accusation words if they made it through
    # (Though primarily we want the LLM to just not generate them)
    
    sanitized = text
    # Append disclaimer
    if DISCLAIMER not in sanitized:
        sanitized += DISCLAIMER
        
    return sanitized
